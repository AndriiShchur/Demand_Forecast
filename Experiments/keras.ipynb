{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('kaggle38': conda)",
   "metadata": {
    "interpreter": {
     "hash": "45e47e45928c796b447ddcb91a7dea648bf57b34fd0dbb45cb997aadc6b2e9c6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Reshape, BatchNormalization, Dropout, concatenate, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from fastai.imports import *\n",
    "# from fastai.column_data import *\n",
    "# from fastai.structured import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_pickle(r'D:\\Project\\Pet_Project\\Demand_Forecast\\Data\\data_2.pkl')\n",
    "\n",
    "test = pd.read_pickle(r'D:\\Project\\Pet_Project\\Demand_Forecast\\Data\\test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "\n",
    "data = data[[\n",
    "    'date_block_num',\n",
    "    'shop_id',\n",
    "    'shop_category',\n",
    "    'item_id',\n",
    "    'item_cnt_month',\n",
    "    'city_code',\n",
    "    'item_category_id',\n",
    "    'type_code',\n",
    "    'subtype_code',\n",
    "    'item_cnt_month_lag_1',\n",
    "    'item_cnt_month_lag_2',\n",
    "    'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_lag_4',\n",
    "    'item_cnt_month_lag_5',\n",
    "    'item_cnt_month_lag_6',\n",
    "    'item_cnt_month_lag_12',\n",
    "    'date_avg_item_cnt_lag_1',\n",
    "    'date_item_avg_item_cnt_lag_1',\n",
    "    'date_item_avg_item_cnt_lag_2',\n",
    "    'date_item_avg_item_cnt_lag_3',\n",
    "    'date_item_avg_item_cnt_lag_6',\n",
    "    'date_item_avg_item_cnt_lag_12',\n",
    "    'date_shop_avg_item_cnt_lag_1',\n",
    "    'date_shop_avg_item_cnt_lag_2',\n",
    "    'date_shop_avg_item_cnt_lag_3',\n",
    "    'date_shop_avg_item_cnt_lag_6',\n",
    "    'date_shop_avg_item_cnt_lag_12',\n",
    "    'date_cat_avg_item_cnt_lag_1',\n",
    "    'date_shop_cat_avg_item_cnt_lag_1',\n",
    "    'date_shop_type_avg_item_cnt_lag_1',\n",
    "    'date_shop_subtype_avg_item_cnt_lag_1',\n",
    "    'date_city_avg_item_cnt_lag_1',\n",
    "    'date_item_city_avg_item_cnt_lag_1',\n",
    "    'date_type_avg_item_cnt_lag_1',\n",
    "    'date_subtype_avg_item_cnt_lag_1',\n",
    "    'delta_price_lag',\n",
    "    'month',\n",
    "    'item_shop_last_sale',\n",
    "    'item_last_sale',\n",
    "    'item_shop_first_sale',\n",
    "    'item_first_sale',\n",
    "    'city_coord_1',\n",
    "    'city_coord_2',\n",
    "    'country_part',\n",
    "    'weeknd_count',\n",
    "    'days_in_month'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature type\n",
    "cat_feature = [\"shop_id\", \"shop_category\", \"item_id\", \"city_code\", \"item_category_id\", \"type_code\", \"subtype_code\", 'month', \"country_part\"]\n",
    "con_feature = ['item_cnt_month_lag_1', 'item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_4', 'item_cnt_month_lag_5', 'item_cnt_month_lag_6', 'item_cnt_month_lag_12', 'date_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_2', 'date_item_avg_item_cnt_lag_3', 'date_item_avg_item_cnt_lag_6', 'date_item_avg_item_cnt_lag_12', 'date_shop_avg_item_cnt_lag_1', 'date_shop_avg_item_cnt_lag_2', 'date_shop_avg_item_cnt_lag_3', 'date_shop_avg_item_cnt_lag_6', 'date_shop_avg_item_cnt_lag_12', 'date_cat_avg_item_cnt_lag_1', 'date_shop_cat_avg_item_cnt_lag_1', 'date_shop_type_avg_item_cnt_lag_1', 'date_shop_subtype_avg_item_cnt_lag_1', 'date_city_avg_item_cnt_lag_1', 'date_item_city_avg_item_cnt_lag_1', 'date_type_avg_item_cnt_lag_1', 'date_subtype_avg_item_cnt_lag_1', 'delta_price_lag', 'item_shop_last_sale', 'item_last_sale','item_shop_first_sale', 'item_first_sale', 'city_coord_1', 'city_coord_2', 'weeknd_count', 'days_in_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "X_train = data[data.date_block_num <= 33]\n",
    "# X_valid = data[data.date_block_num == 33]\n",
    "X_test = data[data.date_block_num == 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Clean env\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(data_df, isTrain=True, shuffle=True):\n",
    "\n",
    "    if shuffle:\n",
    "        data_df = data_df.sample(frac=1)\n",
    "\n",
    "    for cat_f in cat_feature:\n",
    "        data_df[cat_f] = data_df[cat_f].astype(\"category\").cat.as_ordered()\n",
    "\n",
    "    mapper = DataFrameMapper([\n",
    "         (con_feature, StandardScaler())\n",
    "    ])\n",
    "    data_df[con_feature] = mapper.fit_transform(data_df)\n",
    "\n",
    "    label_encoders = []\n",
    "    for f_name in cat_feature:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(data_df[f_name])\n",
    "        label_encoders.append(le)\n",
    "        data_df[f_name] = le.transform(data_df[f_name])\n",
    "\n",
    "    sales_scaler = None\n",
    "    if isTrain:\n",
    "        sales_scaler = StandardScaler()\n",
    "        sales_values = data_df.item_cnt_month.values.reshape(-1,1)\n",
    "        scaled = sales_scaler.fit_transform(sales_values)\n",
    "        data_df.item_cnt_month = scaled\n",
    "    \n",
    "    return data_df, sales_scaler, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         date_block_num  shop_id  shop_category  item_id  item_cnt_month  \\\n",
       "9797431              29        1              1     9760        0.000000   \n",
       "8695944              24       47              0    10387        0.049988   \n",
       "5656821              15       24              0     2080        0.000000   \n",
       "5323490              14       24              0     5396        0.000000   \n",
       "6038554              16       34              1    14449        0.000000   \n",
       "\n",
       "         city_code  item_category_id  type_code  subtype_code  \\\n",
       "9797431          1                50         12             2   \n",
       "8695944         25                64         13            53   \n",
       "5656821         13                52         12             8   \n",
       "5323490         13                26          7            35   \n",
       "6038554         18                37         10             4   \n",
       "\n",
       "         item_cnt_month_lag_1  ...  month  item_shop_last_sale  \\\n",
       "9797431                   0.0  ...      5             0.058824   \n",
       "8695944                   0.0  ...      0             0.058824   \n",
       "5656821                   0.0  ...      3             0.000000   \n",
       "5323490                   0.0  ...      2             0.058824   \n",
       "6038554                   0.0  ...      4             0.000000   \n",
       "\n",
       "         item_last_sale  item_shop_first_sale  item_first_sale  city_coord_1  \\\n",
       "9797431             0.0              0.878788         0.878788      0.899745   \n",
       "8695944             0.0              0.515152         0.515152      0.882553   \n",
       "5656821             0.0              0.363636         0.363636      0.898822   \n",
       "5323490             0.0              0.212121         0.212121      0.898822   \n",
       "6038554             0.0              0.060606         0.181818      0.761451   \n",
       "\n",
       "         city_coord_2  country_part  weeknd_count  days_in_month  \n",
       "9797431      0.292587             1           0.0       0.666667  \n",
       "8695944      0.431409             4           0.0       1.000000  \n",
       "5656821      0.289947             1           0.0       0.666667  \n",
       "5323490      0.289947             1           1.0       1.000000  \n",
       "6038554      0.306194             3           0.0       1.000000  \n",
       "\n",
       "[5 rows x 46 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_block_num</th>\n      <th>shop_id</th>\n      <th>shop_category</th>\n      <th>item_id</th>\n      <th>item_cnt_month</th>\n      <th>city_code</th>\n      <th>item_category_id</th>\n      <th>type_code</th>\n      <th>subtype_code</th>\n      <th>item_cnt_month_lag_1</th>\n      <th>...</th>\n      <th>month</th>\n      <th>item_shop_last_sale</th>\n      <th>item_last_sale</th>\n      <th>item_shop_first_sale</th>\n      <th>item_first_sale</th>\n      <th>city_coord_1</th>\n      <th>city_coord_2</th>\n      <th>country_part</th>\n      <th>weeknd_count</th>\n      <th>days_in_month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9797431</th>\n      <td>29</td>\n      <td>1</td>\n      <td>1</td>\n      <td>9760</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>50</td>\n      <td>12</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0.058824</td>\n      <td>0.0</td>\n      <td>0.878788</td>\n      <td>0.878788</td>\n      <td>0.899745</td>\n      <td>0.292587</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>8695944</th>\n      <td>24</td>\n      <td>47</td>\n      <td>0</td>\n      <td>10387</td>\n      <td>0.049988</td>\n      <td>25</td>\n      <td>64</td>\n      <td>13</td>\n      <td>53</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0.058824</td>\n      <td>0.0</td>\n      <td>0.515152</td>\n      <td>0.515152</td>\n      <td>0.882553</td>\n      <td>0.431409</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>5656821</th>\n      <td>15</td>\n      <td>24</td>\n      <td>0</td>\n      <td>2080</td>\n      <td>0.000000</td>\n      <td>13</td>\n      <td>52</td>\n      <td>12</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.363636</td>\n      <td>0.363636</td>\n      <td>0.898822</td>\n      <td>0.289947</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>5323490</th>\n      <td>14</td>\n      <td>24</td>\n      <td>0</td>\n      <td>5396</td>\n      <td>0.000000</td>\n      <td>13</td>\n      <td>26</td>\n      <td>7</td>\n      <td>35</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>0.058824</td>\n      <td>0.0</td>\n      <td>0.212121</td>\n      <td>0.212121</td>\n      <td>0.898822</td>\n      <td>0.289947</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>6038554</th>\n      <td>16</td>\n      <td>34</td>\n      <td>1</td>\n      <td>14449</td>\n      <td>0.000000</td>\n      <td>18</td>\n      <td>37</td>\n      <td>10</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.060606</td>\n      <td>0.181818</td>\n      <td>0.761451</td>\n      <td>0.306194</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 46 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_df, scaler, label_encoders = prepare_df(X_train)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_model(data_df):\n",
    "    x_fit = []\n",
    "\n",
    "    for cat in cat_feature:\n",
    "        x_fit.append(data_df[cat].values)\n",
    "\n",
    "    for con in con_feature:\n",
    "        x_fit.append(data_df[con].values)\n",
    "        \n",
    "    return x_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = train_df[train_df.date_block_num < 33]\n",
    "x_val_df = train_df[train_df.date_block_num == 33]\n",
    "y_train, y_val = train_df[train_df.date_block_num < 33].item_cnt_month.values, train_df[train_df.date_block_num == 33].item_cnt_month.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fit_train = data_for_model(x_train_df)\n",
    "x_fit_val = data_for_model(x_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(54, 12),\n",
       " (3, 1),\n",
       " (17054, 12),\n",
       " (31, 12),\n",
       " (79, 12),\n",
       " (19, 9),\n",
       " (62, 12),\n",
       " (12, 6),\n",
       " (5, 2)]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "emb_space = [(len(le.classes_), min(25, len(le.classes_)) // 2 ) for idx, le in enumerate(label_encoders)]\n",
    "emb_space"
   ]
  },
  {
   "source": [
    "# Building keras model\n",
    "- Each categorical feature goes thru embedding matrix\n",
    "- Each continues feature (only Elapsed) goes thru simple Dense layer for relu activataion\n",
    "- We add several dense layer and make singe linear output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = []\n",
    "model_embeddings = []\n",
    "    \n",
    "for input_dim, output_dim in emb_space:\n",
    "    i = Input(shape=(1,))\n",
    "    emb = Embedding(input_dim=input_dim, output_dim=output_dim)(i)\n",
    "    \n",
    "    model_inputs.append(i)\n",
    "    model_embeddings.append(emb)\n",
    "    \n",
    "    \n",
    "con_outputs = []\n",
    "for con in con_feature:\n",
    "    elaps_input = Input(shape=(1,))\n",
    "    elaps_output = Dense(10)(elaps_input) \n",
    "    #elaps_output = BatchNormalization()(elaps_output)\n",
    "    elaps_output = Activation(\"relu\")(elaps_output)\n",
    "    \n",
    "    elaps_output = Reshape(target_shape=(1,10))(elaps_output)\n",
    "\n",
    "    model_inputs.append(elaps_input)\n",
    "    con_outputs.append(elaps_output)\n",
    "\n",
    "merge_embeddings = concatenate(model_embeddings, axis=-1)\n",
    "if len(con_outputs) > 1:\n",
    "    merge_con_output = concatenate(con_outputs)\n",
    "else:\n",
    "    merge_con_output = con_outputs[0]\n",
    "\n",
    "merge_embedding_cont = concatenate([merge_embeddings, merge_con_output])\n",
    "merge_embedding_cont\n",
    "\n",
    "output_tensor = Dense(1000, name=\"dense1024\")(merge_embedding_cont)\n",
    "output_tensor = BatchNormalization()(output_tensor)\n",
    "output_tensor = Activation('relu')(output_tensor)\n",
    "#output_tensor = Dropout(0.5)(output_tensor)\n",
    "\n",
    "output_tensor = Dense(500, name=\"dense512\")(output_tensor)\n",
    "output_tensor = BatchNormalization()(output_tensor)\n",
    "output_tensor = Activation(\"relu\")(output_tensor)\n",
    "#output_tensor = Dropout(0.5)(output_tensor)\n",
    "\n",
    "output_tensor = Dense(1, activation='linear', name=\"output\", kernel_constraint = NonNeg())(output_tensor)\n",
    "\n",
    "optimizer = Adam(lr=10e-3)\n",
    "\n",
    "nn_model = Model(inputs=model_inputs, outputs=output_tensor)\n",
    "nn_model.compile(loss='mse', optimizer=optimizer, metrics=['mean_squared_error'])\n",
    "\n",
    "\n",
    "reduceLr=ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, verbose=1)\n",
    "checkpoint = ModelCheckpoint(\"nn_model.hdf5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')#val_mean_absolute_percentage_error\n",
    "callbacks_list = [checkpoint, reduceLr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "6042/6042 [==============================] - 556s 91ms/step - loss: 0.2425 - mean_squared_error: 0.2425 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00222, saving model to nn_model.hdf5\n",
      "Epoch 2/10\n",
      "6042/6042 [==============================] - 463s 77ms/step - loss: 0.0014 - mean_squared_error: 0.0014 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00222 to 0.00215, saving model to nn_model.hdf5\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "Epoch 3/10\n",
      "6042/6042 [==============================] - 591s 98ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "Epoch 4/10\n",
      "6042/6042 [==============================] - 537s 89ms/step - loss: 9.9237e-04 - mean_squared_error: 9.9237e-04 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 7.999999215826393e-05.\n",
      "Epoch 5/10\n",
      "6042/6042 [==============================] - 541s 90ms/step - loss: 9.5766e-04 - mean_squared_error: 9.5766e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.599999814061448e-05.\n",
      "Epoch 6/10\n",
      "6042/6042 [==============================] - 519s 86ms/step - loss: 9.4474e-04 - mean_squared_error: 9.4474e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.199999628122896e-06.\n",
      "Epoch 7/10\n",
      "6042/6042 [==============================] - 552s 91ms/step - loss: 9.4346e-04 - mean_squared_error: 9.4346e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 6.399999165296323e-07.\n",
      "Epoch 8/10\n",
      "6042/6042 [==============================] - 537s 89ms/step - loss: 9.3928e-04 - mean_squared_error: 9.3928e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.2799998785339995e-07.\n",
      "Epoch 9/10\n",
      "6042/6042 [==============================] - 516s 85ms/step - loss: 9.4206e-04 - mean_squared_error: 9.4206e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 2.5599996433811613e-08.\n",
      "Epoch 10/10\n",
      "6042/6042 [==============================] - 519s 86ms/step - loss: 9.4226e-04 - mean_squared_error: 9.4226e-04 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00215\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 5.1199993578165965e-09.\n"
     ]
    }
   ],
   "source": [
    "history = nn_model.fit(x=x_fit_train, y=y_train.reshape(-1,1,1),\n",
    "                       validation_data=(x_fit_val, y_val.reshape(-1,1,1)),\n",
    "                       batch_size=1024, epochs=10, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "tt_model = load_model(r'D:\\Project\\Pet_Project\\Demand_Forecast\\Experiments\\nn_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, _, _ = prepare_df(X_test, isTrain=False, shuffle=False)\n",
    "x_fit_test = data_for_model(test_df)\n",
    "\n",
    "scaled_preds = tt_model.predict(x=x_fit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(214200,)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "scaled_preds.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = scaler.inverse_transform(scaled_preds.reshape(-1, 1))\n",
    "y_predictions = y_predictions.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": y_predictions\n",
    "})\n",
    "submission[submission < 0] = 0\n",
    "submission.to_csv(r'D:\\Project\\Pet_Project\\Demand_Forecast\\Results\\keras_nn_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}